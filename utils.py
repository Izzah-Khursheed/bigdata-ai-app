import pandas as pd
import streamlit as st
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, r2_score
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

from sklearn.preprocessing import LabelEncoder

# ‚úÖ Available models
available_models = {
    "Logistic Regression": LogisticRegression,
    "Linear Regression": LinearRegression,
    "Random Forest": RandomForestClassifier,
    "SVM": SVC,
}

# ‚úÖ Load CSV/Excel with error handling
def load_data(file):
    try:
        if file.name.endswith(".csv"):
            return pd.read_csv(file)
        else:
            return pd.read_excel(file)
    except Exception as e:
        st.error(f"Failed to load data: {e}")
        return None

# ‚úÖ Basic preprocessing: drop NA and encode categorical
def preprocess_data(df):
    df = df.dropna()

    # Encode categorical features
    for col in df.select_dtypes(include='object').columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
    return df

# ‚úÖ Visualizations
def visualize_data(df):
    num_df = df.select_dtypes(include='number')
    
    # Pairplot with Matplotlib
    fig = sns.pairplot(num_df)
    st.pyplot(fig)

    # Plotly histograms
    for col in num_df.columns:
        fig = px.histogram(df, x=col)
        st.plotly_chart(fig)

# ‚úÖ Train model
def train_model(df, target_col, model_name, use_cv=False):
    X = df.drop(target_col, axis=1)
    y = df[target_col]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    model = available_models[model_name]()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return model, X_test, y_test, y_pred

# ‚úÖ Evaluate model
def evaluate_model(model, y_true, y_pred):
    st.subheader("üìä Model Evaluation Summary")

    if is_regression_model(model):
        mse = mean_squared_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)

        st.markdown("### üìà Regression Metrics")
        st.metric(label="Mean Squared Error", value=f"{mse:.2f}")
        st.metric(label="R¬≤ Score", value=f"{r2:.2f}")

    else:
        acc = accuracy_score(y_true, y_pred)
        cm = confusion_matrix(y_true, y_pred)
        report_dict = classification_report(y_true, y_pred, output_dict=True)
        report_df = pd.DataFrame(report_dict).transpose()

        st.markdown("### ‚úÖ Classification Metrics")
        st.metric(label="Accuracy", value=f"{acc:.2%}")

        st.markdown("### üîç Confusion Matrix")
        fig, ax = plt.subplots()
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False, ax=ax)
        ax.set_xlabel("Predicted Labels")
        ax.set_ylabel("True Labels")
        ax.set_title("Confusion Matrix", fontsize=14)
        st.pyplot(fig)

        st.markdown("### üìÑ Classification Report")
        styled_report = report_df.style.background_gradient(cmap='Greens', axis=0).format(precision=2)
        st.dataframe(styled_report)

# ‚úÖ Feature importance (for tree-based models only)
def show_feature_importance(model, X_test):
    if hasattr(model, "feature_importances_"):
        importance = model.feature_importances_
        importance_df = pd.DataFrame({
            'Feature': X_test.columns,
            'Importance': importance
        }).sort_values(by="Importance", ascending=False)

        st.subheader("üîç Feature Importance")
        st.bar_chart(importance_df.set_index("Feature"))
    else:
        st.warning("This model does not support feature importance.")

# ‚úÖ Helper to check if model is for regression
def is_regression_model(model):
    return isinstance(model, LinearRegression)
